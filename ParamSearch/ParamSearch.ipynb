{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "This notebook proposes a parameter searching class for the StarFISH pipeline.\n",
    "\n",
    "# Overview\n",
    "In short, the ParamSearch class runs a StarFISH analysis pipeline and evaluates the results over a grid of parameters. The pipeline is defined using a list of dictionaries and a custom objective function can be applied. ParamSearch should be able to test any pipeline component that adheres to the StarFISH standards. In addition to being useful for visualizing and gaining an intuition for the effect of parameters, ParamSearch may also be a core component in a parameter learning/optimization tool.\n",
    "\n",
    "### Requirements\n",
    "The ParamSearch class should...\n",
    "* evaluate analysis results across a parameter space\n",
    "* be compatible with all filter and spot detection modules in the StarFISH pipeline\n",
    "* support asynchronous concurrent threads\n",
    "\n",
    "### Approach\n",
    "\n",
    "**Inputs:** \n",
    "* stack \\[Stack\\]: Stack object containing the images to be processed\n",
    "* components \\[list\\]: a list of dictionaries describing the modules to include and the parameters to test (in order of execution).\n",
    "\n",
    "**Outputs**\n",
    "* results \\[list\\]: all of the outputs of the objective function.\n",
    "\n",
    "**Public Methods**\n",
    "* constructor:\n",
    "* run(): loops through the grid of parameters and returns the results of the objective functinos\n",
    "\n",
    "**Private Methods**\n",
    "* _attachObjective(objective): saves the objective function to a property. May include some checks in the future.\n",
    "* _generateParams(params): generates the iteratiable grid of parameters\n",
    "* _runPipeline: exectutes one iteration of the pipeline using a set of parameters. Putting this in it's own method should allow for parallelization.\n",
    "\n",
    "# Discussion\n",
    "* Currently, we use ParameterGrid to create an iterable grid of parameters. The input argument is a dict, which works well, but if we have two pipeline steps with the same parameter names, we cannot add both to the same dict. Should we prepend each step's parameters with some sort of step ID to prevent this? It's pretty straightforward to clean when we recall the params under the run method, but perhaps a bit messy.\n",
    "* I haven't implemented the spot detectors in the runPipeline() method because the API hasn't been set. I suspect the way we pass the results to the objective function will be a bit different after spot detection. The plan is to have a switch case based on base class since the pipeline asserts that all spot detectors and filters inherit from their respective base classes.\n",
    "* What do you think is the best way to load the appropriate modules. Currently, we dynamically load them, which ensures only the relevant modules are loaded. I think the Python interpreter is smart and will not reload the module if it has already been loaded, but we should probably verify there isn't significant overhead to this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ParamSearch:\n",
    "    def __init__(self, stack, components, objective):\n",
    "        # todo: add error checking on inputs\n",
    "        \n",
    "        # Save the input parameters\n",
    "        self.stack = stack\n",
    "        self.components = components\n",
    "        \n",
    "        # Create the parameter grid\n",
    "        self._createParamGrid()\n",
    "        \n",
    "        # Attach the objective function\n",
    "        self._attachObjective(objective)\n",
    "        \n",
    "    def run(self):\n",
    "        # Check if the param grid has been made\n",
    "        if self.param_grid is None:\n",
    "            # TODO: raise error\n",
    "            print('do the param grid')\n",
    "            \n",
    "        # TODO: add option to use concurrent futures module\n",
    "        # Run through the pipeline\n",
    "        results = []\n",
    "        \n",
    "        for params in tqdm(self.param_grid):    \n",
    "            results.append(self._runPipeline(deepcopy(self.stack), params))\n",
    "        \n",
    "        #results = self._runParallel()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _runParallel(self):\n",
    "        # TODO: test/fix\n",
    "        import concurrent.futures\n",
    "        from itertools import repeat\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            results =  executor.map(self._runPipeline(), repeat(deepcopy(self.stack), len(self.param_grid)), self.param_grid)\n",
    "                \n",
    "        return results\n",
    "            \n",
    "    def _attachObjective(self, fcn):\n",
    "        self.objective = fcn\n",
    "        \n",
    "    def _createParamGrid(self):\n",
    "        from sklearn.model_selection import ParameterGrid\n",
    "        \n",
    "        # Create a dictionary of all params from the components list\n",
    "        # TODO: prepend UID for each pipeline step to prevent collision of keys\n",
    "        self._params = {}\n",
    "        for component in self.components:\n",
    "            self._params.update(component['parameters'])\n",
    "        \n",
    "        # Create the iterable grid of parameters\n",
    "        self.param_grid = ParameterGrid(self._params)\n",
    "    \n",
    "    def _runPipeline(self, stack, params):\n",
    "        # Run through the pipeline\n",
    "        \n",
    "        for idx, component in enumerate(self.components):\n",
    "            # Get the parameters for this module\n",
    "            # TODO: add UID for each pipeline step to prevent collision of keys\n",
    "            keys = list(component['parameters'].keys())\n",
    "            component_params = dict((k, params[k]) for k in keys)\n",
    "            \n",
    "            # Get the pipeline component class\n",
    "            cls = getattr(import_module(component['module']),\n",
    "                         component['class'])\n",
    "            \n",
    "            # Instantiate the pipeline component object\n",
    "            obj = cls(**component_params)\n",
    "            \n",
    "            # Perform the operation on the image\n",
    "            # TODO: add ability to run spot detectors - need interface to be unified\n",
    "            obj.filter(stack)\n",
    "            \n",
    "        # Evaluate the result\n",
    "        res = self.objective(stack)\n",
    "        \n",
    "        return res\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition\n",
    "The pipeline is defined as a list of dictionaries that describe each component. Each dictionary must have the following keys:\n",
    "* module: a string with the module to load\n",
    "* class: the name of the class to load\n",
    "* params: a dictionary where each key is a parameter name and each value is a list of parameters to create the grid across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_step1 = {'module': 'starfish.pipeline.filter.white_tophat',\n",
    "                 'class': 'WhiteTophat',\n",
    "                 'parameters': {'disk_size': [1, 2, 3]}}\n",
    "\n",
    "\n",
    "components = [pipeline_step1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function definition\n",
    "Here we define a function to evaluate the results of each set of parameters. For now, I have set it as the number of spots detected. In the future if we have some reference data (e.g., a crowd-sourced annotation), we can calculate precision/recall. I look forward to discussing the best metrics to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackpy import locate\n",
    "from showit import image\n",
    "\n",
    "def testObjective(stack):\n",
    "    ch1 = stack.image.max_proj(Indices.Z)[0, 0]\n",
    "    \n",
    "    results = locate(ch1, diameter=3, minmass=7500, maxsize=3, separation=5, preprocess=False, percentile=10) \n",
    "    results.columns = ['y', 'x', 'intensity', 'r', 'eccentricity', 'signal', 'raw_mass', 'ep']\n",
    "\n",
    "    return len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the run method\n",
    "To run the class, simply load an image into a Stack, pass the Stack object, pipeline components, and objective function to the ParamSearch constructor and call the run() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.io import Stack\n",
    "from starfish.constants import Indices\n",
    "\n",
    "# Load the test images\n",
    "experiment_json = './output/experiment.json'\n",
    "\n",
    "s = Stack()\n",
    "s.read(experiment_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 331, 1400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "searcher = ParamSearch(s, components, testObjective)\n",
    "n_spots = searcher.run()\n",
    "\n",
    "print(n_spots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
